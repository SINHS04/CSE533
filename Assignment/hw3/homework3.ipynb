{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions from _01_code/_99_common_utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_linux():\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_windows():\n",
    "    if os.name == \"nt\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_mac():\n",
    "    if sys.platform == \"darwin\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_num_cpu_cores():\n",
    "    import multiprocessing\n",
    "    return multiprocessing.cpu_count()\n",
    "\n",
    "class DeltaTemplate(Template):\n",
    "    delimiter = \"%\"\n",
    "\n",
    "    def strfdelta(tdelta, fmt):\n",
    "        d = {\"D\": tdelta.days}\n",
    "        d[\"H\"], rem = divmod(tdelta.seconds, 3600)\n",
    "        d[\"M\"], d[\"S\"] = divmod(rem, 60)\n",
    "        t = DeltaTemplate(fmt)\n",
    "        return t.substitute(**d)\n",
    "\n",
    "def strfdelta(td, fmt):\n",
    "\n",
    "    # Get the timedelta’s sign and absolute number of seconds.\n",
    "    sign = \"-\" if td.days < 0 else \"+\"\n",
    "    secs = abs(td).total_seconds()\n",
    "\n",
    "    # Break the seconds into more readable quantities.\n",
    "    days, rem = divmod(secs, 86400)  # Seconds per day: 24 * 60 * 60\n",
    "    hours, rem = divmod(rem, 3600)  # Seconds per hour: 60 * 60\n",
    "    mins, secs = divmod(rem, 60)\n",
    "\n",
    "    # Format (as per above answers) and return the result string.\n",
    "    t = DeltaTemplate(fmt)\n",
    "    return t.substitute(\n",
    "        s=sign,\n",
    "        D=\"{:d}\".format(int(days)),\n",
    "        H=\"{:02d}\".format(int(hours)),\n",
    "        M=\"{:02d}\".format(int(mins)),\n",
    "        S=\"{:02d}\".format(int(secs)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제 1] Fashion MNIST 데이터 정규화를 위한 Mean과 Std값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment function\n",
    "def augment_f_mnist_train(f_mnist_train):\n",
    "    f_mnist_train_transforms = nn.Sequential(\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop([28, 28], padding=4),\n",
    "    )\n",
    "\n",
    "    transformed_train_data = []\n",
    "\n",
    "    for image, label in f_mnist_train:\n",
    "        transformed_image = f_mnist_train_transforms(image)\n",
    "        transformed_train_data.append((transformed_image, label))\n",
    "\n",
    "    f_mnist_train = ConcatDataset([f_mnist_train, transformed_train_data])\n",
    "\n",
    "    return f_mnist_train\n",
    "\n",
    "def get_f_mnist_train_data():\n",
    "    data_path = \".\" \n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    ## augment ##\n",
    "    f_mnist_train = augment_f_mnist_train(f_mnist_train)\n",
    "    ##############\n",
    "\n",
    "    imgs = [img for img, _ in f_mnist_train]\n",
    "    imgs = torch.concat(imgs, dim=0)\n",
    "    mean = imgs.mean(dim=[0, 1, 2])\n",
    "    std = imgs.std(dim=[0, 1, 2])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    )\n",
    "    \n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_mnist_test_data(mean, std):\n",
    "    data_path = \".\"\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    )\n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNet(CNN) Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_googlenet_model():\n",
    "    class Inception(nn.Module):\n",
    "        def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "            super(Inception, self).__init__(**kwargs)\n",
    "            self.b1_1 = nn.LazyConv2d(out_channels=c1, kernel_size=1)\n",
    "\n",
    "            self.b2_1 = nn.LazyConv2d(out_channels=c2[0], kernel_size=1)\n",
    "            self.b2_2 = nn.LazyConv2d(out_channels=c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "            self.b3_1 = nn.LazyConv2d(out_channels=c3[0], kernel_size=1)\n",
    "            self.b3_2 = nn.LazyConv2d(out_channels=c3[1], kernel_size=5, padding=2)\n",
    "\n",
    "            self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "            self.b4_2 = nn.LazyConv2d(out_channels=c4, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            b1 = torch.relu(self.b1_1(x))\n",
    "            b2 = torch.relu(self.b2_2(torch.relu(self.b2_1(x))))\n",
    "            b3 = torch.relu(self.b3_2(torch.relu(self.b3_1(x))))\n",
    "            b4 = torch.relu(self.b4_2(self.b4_1(x)))\n",
    "            return torch.cat((b1, b2, b3, b4), dim=1)\n",
    "        \n",
    "    class InceptionAux(nn.Module):\n",
    "        def __init__(self, n_outputs, **kwargs):\n",
    "            super(InceptionAux, self).__init__(**kwargs)\n",
    "\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=128, kernel_size=1),\n",
    "            )\n",
    "\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.LazyLinear(out_features=1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "                nn.LazyLinear(out_features=n_outputs),\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "        \n",
    "    class GoogleNet(nn.Module):\n",
    "        def __init__(self, n_outputs=10):\n",
    "            super(GoogleNet, self).__init__()\n",
    "            self.conv_block = nn.Sequential(self.conv_blk_1(), self.conv_blk_2())\n",
    "            self.inception_block_1 = self.inception_blk_1()\n",
    "            self.inception_block_2 = self.inception_blk_2()\n",
    "            self.inception_block_3 = self.inception_blk_3()\n",
    "            self.aux_1 = InceptionAux(n_outputs)\n",
    "            self.aux_2 = InceptionAux(n_outputs)\n",
    "\n",
    "        def conv_blk_1(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "        def conv_blk_2(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=64, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.LazyConv2d(out_channels=192, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "        def inception_blk_1(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=64, c2=(96, 128), c3=(16, 32), c4=32),\n",
    "                Inception(c1=128, c2=(128, 192), c3=(32, 96), c4=64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                Inception(c1=192, c2=(96, 208), c3=(16, 48), c4=64),\n",
    "            )\n",
    "        \n",
    "        def inception_blk_2(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=160, c2=(112, 224), c3=(24, 64), c4=64),\n",
    "                Inception(c1=128, c2=(128, 256), c3=(24, 64), c4=64),\n",
    "                Inception(c1=112, c2=(144, 288), c3=(32, 64), c4=64),\n",
    "            )\n",
    "        \n",
    "        def inception_blk_3(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "                Inception(c1=384, c2=(192, 384), c3=(48, 128), c4=128),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv_block(x)\n",
    "            x = self.inception_block_1(x)\n",
    "            aux_out_1 = self.aux_1(x)\n",
    "            x = self.inception_block_2(x)\n",
    "            aux_out_2 = self.aux_2(x)\n",
    "            x = self.inception_block_3(x)\n",
    "            return x, aux_out_1, aux_out_2\n",
    "\n",
    "    my_model = GoogleNet()\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "  def __init__(self, patience=10, delta=0.00001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "    self.patience = patience\n",
    "    self.counter = 0\n",
    "    self.delta = delta\n",
    "\n",
    "    self.val_loss_min = None\n",
    "    self.file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "    )\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "  def check_and_save(self, new_validation_loss, model):\n",
    "    early_stop = False\n",
    "\n",
    "    if self.val_loss_min is None:\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      message = f'Early stopping is stated!'\n",
    "    elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "      message = f'V_loss decreased ({self.val_loss_min:7.5f} --> {new_validation_loss:7.5f}). Saving model...'\n",
    "      self.save_checkpoint(new_validation_loss, model)\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "      if self.counter >= self.patience:\n",
    "        early_stop = True\n",
    "        message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "\n",
    "    return message, early_stop\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    '''Saves model when validation loss decrease.'''\n",
    "    torch.save(model.state_dict(), self.file_path)\n",
    "    torch.save(model.state_dict(), self.latest_file_path)\n",
    "    self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTester:\n",
    "  def __init__(self, project_name, model, test_data_loader, transforms, checkpoint_file_path):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.test_data_loader = test_data_loader\n",
    "    self.transforms = transforms\n",
    "\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "    print(\"MODEL FILE: {0}\".format(self.latest_file_path))\n",
    "\n",
    "    self.model.load_state_dict(torch.load(self.latest_file_path, map_location=torch.device('cpu')))\n",
    "\n",
    "  def test(self):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    num_corrects_test = 0\n",
    "    num_tested_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for test_batch in self.test_data_loader:\n",
    "        input_test, target_test = test_batch\n",
    "\n",
    "        input_test = self.transforms(input_test)\n",
    "\n",
    "        output_test, _, _ = self.model(input_test)\n",
    "\n",
    "        predicted_test = torch.argmax(output_test, dim=1)\n",
    "        num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "        num_tested_samples += len(input_test)\n",
    "\n",
    "      test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "    print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "  def test_single(self, input_test):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    with torch.no_grad():\n",
    "      input_test = self.transforms(input_test)\n",
    "\n",
    "      output_test, _, _ = self.model(input_test)\n",
    "      predicted_test = torch.argmax(output_test, dim=1)\n",
    "\n",
    "    return predicted_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNetClassificationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNetClassificationTrainer(ClassificationTrainer):\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    super(GoogLeNetClassificationTrainer, self).__init__(\n",
    "      project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "      run_time_str, wandb, device, checkpoint_file_path\n",
    "    )\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train, output_train_ax_1, output_train_ax_2 = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_aux_1 = self.loss_fn(output_train_ax_1, target_train)\n",
    "      loss_aux_2 = self.loss_fn(output_train_ax_2, target_train)\n",
    "      loss += 0.3 * (loss_aux_1 + loss_aux_2)\n",
    "\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation, output_validation_ax_1, output_validation_ax_2 = self.model(input_validation)\n",
    "        loss_validation = self.loss_fn(output_validation, target_validation)\n",
    "        loss_validation_aux_1 = self.loss_fn(output_validation_ax_1, target_validation)\n",
    "        loss_validation_aux_2 = self.loss_fn(output_validation_ax_2, target_validation)\n",
    "        loss_validation += 0.3 * (loss_validation_aux_1 + loss_validation_aux_2)\n",
    "        loss_validation += loss_validation.item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(config):\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms, mean, std = get_f_mnist_train_data()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = get_googlenet_model()\n",
    "    best_path = 'checkpoints/CSE533_Fashion_MNIST_checkpoint_2023-11-18_00-17-29.pt'\n",
    "    model.load_state_dict(torch.load(best_path, map_location=torch.device('cpu')))\n",
    "    model.to(device)\n",
    "\n",
    "    summary(\n",
    "        model = model,\n",
    "        input_size=(1, 1, 28, 28),\n",
    "        col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\", \"mult_adds\"]\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=wandb.config.learning_rate,\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "\n",
    "    classification_trainer = GoogLeNetClassificationTrainer(\n",
    "        config['project'], model, optimizer, train_data_loader, validation_data_loader, f_mnist_transforms,\n",
    "        config['current_time_str'], wandb, device, config['checkpoints_path']\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(config, mean, std):\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_f_mnist_test_data(mean, std)\n",
    "\n",
    "    test_model = get_googlenet_model()\n",
    "\n",
    "    classification_tester = ClassificationTester(\n",
    "        config['project'], test_model, test_data_loader, f_mnist_transforms, config['checkpoints_path']\n",
    "    )\n",
    "    classification_tester.test()\n",
    "\n",
    "    img, label = f_mnist_test_images[0]\n",
    "    print(\"     LABEL:\", label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    output = classification_tester.test_single(\n",
    "        torch.tensor(np.array(f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    )\n",
    "    print(\"PREDICTION:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mckw6896\u001b[0m (\u001b[33mdicelab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nlp-06/CSE533/assignment/CSE533/Assignment/hw3/wandb/run-20231118_042801-e64no61t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t' target=\"_blank\">2023-11-18_04-28-00</a></strong> to <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 10000, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 2e-05, 'early_stop_patience': 10, 'early_stop_delta': 1e-05, 'weight_decay': 0, 'project': 'CSE533_Fashion_MNIST', 'use_wandb': True, 'current_time_str': '2023-11-18_04-28-00', 'checkpoints_path': 'checkpoints'}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    config = {\n",
    "        'epochs': 10_000,\n",
    "        'batch_size': 2048,\n",
    "        'validation_intervals': 10,\n",
    "        'learning_rate': 2e-5,\n",
    "        'early_stop_patience': 10,\n",
    "        'early_stop_delta': 0.00001,\n",
    "        'weight_decay': 0,\n",
    "        'project': \"CSE533_Fashion_MNIST\",\n",
    "        'use_wandb': True,\n",
    "        'current_time_str': datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        'checkpoints_path': \"checkpoints\"\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(config['checkpoints_path']):\n",
    "        os.makedirs(config['checkpoints_path'])\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if config['use_wandb'] else \"disabled\",\n",
    "        project=config['project'],\n",
    "        notes=\"Homework3\",\n",
    "        tags=['googlenet', 'FashionMNIST'],\n",
    "        name=config['current_time_str'],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    print(wandb.config)\n",
    "\n",
    "    mean = 0.3\n",
    "    std = 0.2\n",
    "\n",
    "    try:\n",
    "        mean, std = model_train(config)\n",
    "    except:\n",
    "        wandb.finish()\n",
    "        return\n",
    "        \n",
    "    model_test(config, mean, std)\n",
    "\n",
    "    print(\"#### Q1 Answer ####\")\n",
    "    print(\"Fashion Mnist Train Data's Mean\\t:\", mean)\n",
    "    print(\"Fashion Mnist Train Data's Std\\t:\", std)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE533",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
