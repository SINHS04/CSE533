{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions from _01_code/_99_common_utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_linux():\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_windows():\n",
    "    if os.name == \"nt\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_mac():\n",
    "    if sys.platform == \"darwin\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_num_cpu_cores():\n",
    "    import multiprocessing\n",
    "    return multiprocessing.cpu_count()\n",
    "\n",
    "class DeltaTemplate(Template):\n",
    "    delimiter = \"%\"\n",
    "\n",
    "    def strfdelta(tdelta, fmt):\n",
    "        d = {\"D\": tdelta.days}\n",
    "        d[\"H\"], rem = divmod(tdelta.seconds, 3600)\n",
    "        d[\"M\"], d[\"S\"] = divmod(rem, 60)\n",
    "        t = DeltaTemplate(fmt)\n",
    "        return t.substitute(**d)\n",
    "\n",
    "def strfdelta(td, fmt):\n",
    "\n",
    "    # Get the timedelta’s sign and absolute number of seconds.\n",
    "    sign = \"-\" if td.days < 0 else \"+\"\n",
    "    secs = abs(td).total_seconds()\n",
    "\n",
    "    # Break the seconds into more readable quantities.\n",
    "    days, rem = divmod(secs, 86400)  # Seconds per day: 24 * 60 * 60\n",
    "    hours, rem = divmod(rem, 3600)  # Seconds per hour: 60 * 60\n",
    "    mins, secs = divmod(rem, 60)\n",
    "\n",
    "    # Format (as per above answers) and return the result string.\n",
    "    t = DeltaTemplate(fmt)\n",
    "    return t.substitute(\n",
    "        s=sign,\n",
    "        D=\"{:d}\".format(int(days)),\n",
    "        H=\"{:02d}\".format(int(hours)),\n",
    "        M=\"{:02d}\".format(int(mins)),\n",
    "        S=\"{:02d}\".format(int(secs)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제 1] Fashion MNIST 데이터 정규화를 위한 Mean과 Std값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment function\n",
    "def augment_f_mnist_train(f_mnist_train):\n",
    "    f_mnist_train_transforms = nn.Sequential(\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop([28, 28], padding=4),\n",
    "    )\n",
    "\n",
    "    transformed_train_data = []\n",
    "\n",
    "    for image, label in f_mnist_train:\n",
    "        transformed_image = f_mnist_train_transforms(image)\n",
    "        transformed_train_data.append((transformed_image, label))\n",
    "\n",
    "    f_mnist_train = ConcatDataset([f_mnist_train, transformed_train_data])\n",
    "\n",
    "    return f_mnist_train\n",
    "\n",
    "def get_f_mnist_train_data():\n",
    "    data_path = \".\" \n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    ## augment ##\n",
    "    # f_mnist_train = augment_f_mnist_train(f_mnist_train)\n",
    "    ##############\n",
    "\n",
    "    imgs = [img for img, _ in f_mnist_train]\n",
    "    imgs = torch.concat(imgs, dim=0)\n",
    "    mean = imgs.mean(dim=[0, 1, 2])\n",
    "    std = imgs.std(dim=[0, 1, 2])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    )\n",
    "    \n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_mnist_test_data(mean, std):\n",
    "    data_path = \".\"\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    )\n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNet(CNN) Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_googlenet_model():\n",
    "    class Inception(nn.Module):\n",
    "        def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "            super(Inception, self).__init__(**kwargs)\n",
    "            self.b1_1 = nn.LazyConv2d(out_channels=c1, kernel_size=1)\n",
    "\n",
    "            self.b2_1 = nn.LazyConv2d(out_channels=c2[0], kernel_size=1)\n",
    "            self.b2_2 = nn.LazyConv2d(out_channels=c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "            self.b3_1 = nn.LazyConv2d(out_channels=c3[0], kernel_size=1)\n",
    "            self.b3_2 = nn.LazyConv2d(out_channels=c3[1], kernel_size=5, padding=2)\n",
    "\n",
    "            self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "            self.b4_2 = nn.LazyConv2d(out_channels=c4, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            b1 = torch.relu(self.b1_1(x))\n",
    "            b2 = torch.relu(self.b2_2(torch.relu(self.b2_1(x))))\n",
    "            b3 = torch.relu(self.b3_2(torch.relu(self.b3_1(x))))\n",
    "            b4 = torch.relu(self.b4_2(self.b4_1(x)))\n",
    "            return torch.cat((b1, b2, b3, b4), dim=1)\n",
    "        \n",
    "    class InceptionAux(nn.Module):\n",
    "        def __init__(self, n_outputs, **kwargs):\n",
    "            super(InceptionAux, self).__init__(**kwargs)\n",
    "\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=128, kernel_size=1),\n",
    "            )\n",
    "\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.LazyLinear(out_features=1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "                nn.LazyLinear(out_features=n_outputs),\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "        \n",
    "    class GoogleNet(nn.Module):\n",
    "        def __init__(self, n_outputs=10):\n",
    "            super(GoogleNet, self).__init__()\n",
    "            self.conv_block = nn.Sequential(self.conv_blk_1(), self.conv_blk_2())\n",
    "            self.inception_block_1 = self.inception_blk_1()\n",
    "            self.inception_block_2 = self.inception_blk_2()\n",
    "            self.inception_block_3 = self.inception_blk_3()\n",
    "            self.aux_1 = InceptionAux(n_outputs)\n",
    "            self.aux_2 = InceptionAux(n_outputs)\n",
    "\n",
    "        def conv_blk_1(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "        def conv_blk_2(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=64, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.LazyConv2d(out_channels=192, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "        def inception_blk_1(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=64, c2=(96, 128), c3=(16, 32), c4=32),\n",
    "                Inception(c1=128, c2=(128, 192), c3=(32, 96), c4=64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                Inception(c1=192, c2=(96, 208), c3=(16, 48), c4=64),\n",
    "            )\n",
    "        \n",
    "        def inception_blk_2(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=160, c2=(112, 224), c3=(24, 64), c4=64),\n",
    "                Inception(c1=128, c2=(128, 256), c3=(24, 64), c4=64),\n",
    "                Inception(c1=112, c2=(144, 288), c3=(32, 64), c4=64),\n",
    "            )\n",
    "        \n",
    "        def inception_blk_3(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "                Inception(c1=384, c2=(192, 384), c3=(48, 128), c4=128),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv_block(x)\n",
    "            x = self.inception_block_1(x)\n",
    "            aux_out_1 = self.aux_1(x)\n",
    "            x = self.inception_block_2(x)\n",
    "            aux_out_2 = self.aux_2(x)\n",
    "            x = self.inception_block_3(x)\n",
    "            return x, aux_out_1, aux_out_2\n",
    "\n",
    "    my_model = GoogleNet()\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "  def __init__(self, patience=10, delta=0.00001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "    self.patience = patience\n",
    "    self.counter = 0\n",
    "    self.delta = delta\n",
    "\n",
    "    self.val_loss_min = None\n",
    "    self.file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "    )\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "  def check_and_save(self, new_validation_loss, model):\n",
    "    early_stop = False\n",
    "\n",
    "    if self.val_loss_min is None:\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      message = f'Early stopping is stated!'\n",
    "    elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "      message = f'V_loss decreased ({self.val_loss_min:7.5f} --> {new_validation_loss:7.5f}). Saving model...'\n",
    "      self.save_checkpoint(new_validation_loss, model)\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "      if self.counter >= self.patience:\n",
    "        early_stop = True\n",
    "        message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "\n",
    "    return message, early_stop\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    '''Saves model when validation loss decrease.'''\n",
    "    torch.save(model.state_dict(), self.file_path)\n",
    "    torch.save(model.state_dict(), self.latest_file_path)\n",
    "    self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTester:\n",
    "  def __init__(self, project_name, model, test_data_loader, transforms, checkpoint_file_path):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.test_data_loader = test_data_loader\n",
    "    self.transforms = transforms\n",
    "\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "    print(\"MODEL FILE: {0}\".format(self.latest_file_path))\n",
    "\n",
    "    self.model.load_state_dict(torch.load(self.latest_file_path, map_location=torch.device('cpu')))\n",
    "\n",
    "  def test(self):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    num_corrects_test = 0\n",
    "    num_tested_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for test_batch in self.test_data_loader:\n",
    "        input_test, target_test = test_batch\n",
    "\n",
    "        input_test = self.transforms(input_test)\n",
    "\n",
    "        output_test, _, _ = self.model(input_test)\n",
    "\n",
    "        predicted_test = torch.argmax(output_test, dim=1)\n",
    "        num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "        num_tested_samples += len(input_test)\n",
    "\n",
    "      test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "    print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "  def test_single(self, input_test):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    with torch.no_grad():\n",
    "      input_test = self.transforms(input_test)\n",
    "\n",
    "      output_test = self.model(input_test)\n",
    "      predicted_test = torch.argmax(output_test, dim=1)\n",
    "\n",
    "    return predicted_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNetClassificationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNetClassificationTrainer(ClassificationTrainer):\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    super(GoogLeNetClassificationTrainer, self).__init__(\n",
    "      project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "      run_time_str, wandb, device, checkpoint_file_path\n",
    "    )\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train, output_train_ax_1, output_train_ax_2 = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_aux_1 = self.loss_fn(output_train_ax_1, target_train)\n",
    "      loss_aux_2 = self.loss_fn(output_train_ax_2, target_train)\n",
    "      loss += 0.3 * (loss_aux_1 + loss_aux_2)\n",
    "\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation, output_validation_ax_1, output_validation_ax_2 = self.model(input_validation)\n",
    "        loss_validation = self.loss_fn(output_validation, target_validation)\n",
    "        loss_validation_aux_1 = self.loss_fn(output_validation_ax_1, target_validation)\n",
    "        loss_validation_aux_2 = self.loss_fn(output_validation_ax_2, target_validation)\n",
    "        loss_validation += 0.3 * (loss_validation_aux_1 + loss_validation_aux_2)\n",
    "        loss_validation += loss_validation.item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(config):\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms, mean, std = get_f_mnist_train_data()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = get_googlenet_model()\n",
    "    model.to(device)\n",
    "\n",
    "    summary(\n",
    "        model = model,\n",
    "        input_size=(1, 1, 28, 28),\n",
    "        col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\", \"mult_adds\"]\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=wandb.config.learning_rate,\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "\n",
    "    classification_trainer = GoogLeNetClassificationTrainer(\n",
    "        config['project'], model, optimizer, train_data_loader, validation_data_loader, f_mnist_transforms,\n",
    "        config['current_time_str'], wandb, device, config['checkpoints_path']\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(config, mean, std):\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_f_mnist_test_data(mean, std)\n",
    "\n",
    "    test_model = get_googlenet_model()\n",
    "\n",
    "    classification_tester = ClassificationTester(\n",
    "        config['project'], test_model, test_data_loader, f_mnist_transforms, config['checkpoints_path']\n",
    "    )\n",
    "    classification_tester.test()\n",
    "\n",
    "    img, label = f_mnist_test_images[0]\n",
    "    print(\"     LABEL:\", label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    output = classification_tester.test_single(\n",
    "    torch.tensor(np.array(f_mnist_test_images[0][0])).permute(2, 0, 1).unsqueeze(dim=0)\n",
    "    )\n",
    "    print(\"PREDICTION:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nlp-06/CSE533/assignment/CSE533/Assignment/hw3/wandb/run-20231118_023307-xwhvfu9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/xwhvfu9g' target=\"_blank\">2023-11-18_02-33-07</a></strong> to <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/xwhvfu9g' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/xwhvfu9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 10000, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 2e-05, 'early_stop_patience': 5, 'early_stop_delta': 1e-05, 'weight_decay': 0.001, 'project': 'CSE533_Fashion_MNIST', 'use_wandb': True, 'current_time_str': '2023-11-18_02-33-07', 'checkpoints_path': 'checkpoints'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-06/anaconda3/envs/CSE533/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1] T_loss: 8.31178, T_accuracy: 5.5709 | V_loss: 5.53826, V_accuracy: 10.2200 | Early stopping is stated! | T_time: 00:00:08, T_speed: 0.125\n",
      "[Epoch  10] T_loss: 4.81116, T_accuracy: 9.7673 | V_loss: 3.04385, V_accuracy: 10.0000 | V_loss decreased (5.53826 --> 3.04385). Saving model... | T_time: 00:01:07, T_speed: 0.149\n",
      "[Epoch  20] T_loss: 4.09834, T_accuracy: 10.0000 | V_loss: 2.72199, V_accuracy: 10.0000 | V_loss decreased (3.04385 --> 2.72199). Saving model... | T_time: 00:02:14, T_speed: 0.149\n",
      "[Epoch  30] T_loss: 3.38416, T_accuracy: 17.9982 | V_loss: 2.28484, V_accuracy: 22.9800 | V_loss decreased (2.72199 --> 2.28484). Saving model... | T_time: 00:03:20, T_speed: 0.150\n",
      "[Epoch  40] T_loss: 2.65763, T_accuracy: 48.7273 | V_loss: 1.82221, V_accuracy: 49.4600 | V_loss decreased (2.28484 --> 1.82221). Saving model... | T_time: 00:04:27, T_speed: 0.150\n",
      "[Epoch  50] T_loss: 2.35784, T_accuracy: 57.6018 | V_loss: 1.62687, V_accuracy: 57.0600 | V_loss decreased (1.82221 --> 1.62687). Saving model... | T_time: 00:05:33, T_speed: 0.150\n",
      "[Epoch  60] T_loss: 2.19121, T_accuracy: 63.7727 | V_loss: 1.51207, V_accuracy: 64.4800 | V_loss decreased (1.62687 --> 1.51207). Saving model... | T_time: 00:06:40, T_speed: 0.150\n",
      "[Epoch  70] T_loss: 2.04983, T_accuracy: 68.2291 | V_loss: 1.42563, V_accuracy: 68.3200 | V_loss decreased (1.51207 --> 1.42563). Saving model... | T_time: 00:07:46, T_speed: 0.150\n",
      "[Epoch  80] T_loss: 1.94741, T_accuracy: 70.7600 | V_loss: 1.36758, V_accuracy: 71.1400 | V_loss decreased (1.42563 --> 1.36758). Saving model... | T_time: 00:08:52, T_speed: 0.150\n",
      "[Epoch  90] T_loss: 1.87214, T_accuracy: 72.6400 | V_loss: 1.30069, V_accuracy: 72.7800 | V_loss decreased (1.36758 --> 1.30069). Saving model... | T_time: 00:09:59, T_speed: 0.150\n",
      "[Epoch 100] T_loss: 1.80055, T_accuracy: 74.4109 | V_loss: 1.24957, V_accuracy: 73.8200 | V_loss decreased (1.30069 --> 1.24957). Saving model... | T_time: 00:11:05, T_speed: 0.150\n",
      "[Epoch 110] T_loss: 1.75411, T_accuracy: 75.4800 | V_loss: 1.21975, V_accuracy: 74.7600 | V_loss decreased (1.24957 --> 1.21975). Saving model... | T_time: 00:12:12, T_speed: 0.150\n",
      "[Epoch 120] T_loss: 1.71669, T_accuracy: 76.2000 | V_loss: 1.20204, V_accuracy: 75.3400 | V_loss decreased (1.21975 --> 1.20204). Saving model... | T_time: 00:13:18, T_speed: 0.150\n",
      "[Epoch 130] T_loss: 1.69150, T_accuracy: 76.8073 | V_loss: 1.16858, V_accuracy: 76.1400 | V_loss decreased (1.20204 --> 1.16858). Saving model... | T_time: 00:14:25, T_speed: 0.150\n",
      "[Epoch 140] T_loss: 1.65997, T_accuracy: 77.4091 | V_loss: 1.15430, V_accuracy: 76.5200 | V_loss decreased (1.16858 --> 1.15430). Saving model... | T_time: 00:15:33, T_speed: 0.150\n",
      "[Epoch 150] T_loss: 1.63663, T_accuracy: 77.8873 | V_loss: 1.14157, V_accuracy: 76.8400 | V_loss decreased (1.15430 --> 1.14157). Saving model... | T_time: 00:16:39, T_speed: 0.150\n",
      "[Epoch 160] T_loss: 1.61638, T_accuracy: 78.2764 | V_loss: 1.14128, V_accuracy: 77.0000 | V_loss decreased (1.14157 --> 1.14128). Saving model... | T_time: 00:17:46, T_speed: 0.150\n",
      "[Epoch 170] T_loss: 1.59265, T_accuracy: 78.7855 | V_loss: 1.11583, V_accuracy: 77.2400 | V_loss decreased (1.14128 --> 1.11583). Saving model... | T_time: 00:18:52, T_speed: 0.150\n",
      "[Epoch 180] T_loss: 1.58563, T_accuracy: 78.7800 | V_loss: 1.11592, V_accuracy: 77.5000 | Early stopping counter: 1 out of 5 | T_time: 00:19:59, T_speed: 0.150\n",
      "[Epoch 190] T_loss: 1.55857, T_accuracy: 79.4145 | V_loss: 1.09640, V_accuracy: 77.6000 | V_loss decreased (1.11583 --> 1.09640). Saving model... | T_time: 00:21:06, T_speed: 0.150\n",
      "[Epoch 200] T_loss: 1.54692, T_accuracy: 79.5400 | V_loss: 1.08380, V_accuracy: 77.7600 | V_loss decreased (1.09640 --> 1.08380). Saving model... | T_time: 00:22:12, T_speed: 0.150\n",
      "[Epoch 210] T_loss: 1.52288, T_accuracy: 80.0436 | V_loss: 1.08032, V_accuracy: 78.2000 | V_loss decreased (1.08380 --> 1.08032). Saving model... | T_time: 00:23:18, T_speed: 0.150\n",
      "[Epoch 220] T_loss: 1.51425, T_accuracy: 80.0382 | V_loss: 1.05380, V_accuracy: 78.3800 | V_loss decreased (1.08032 --> 1.05380). Saving model... | T_time: 00:24:25, T_speed: 0.150\n",
      "[Epoch 230] T_loss: 1.48962, T_accuracy: 80.4909 | V_loss: 1.04963, V_accuracy: 78.5000 | V_loss decreased (1.05380 --> 1.04963). Saving model... | T_time: 00:25:31, T_speed: 0.150\n",
      "[Epoch 240] T_loss: 1.48578, T_accuracy: 80.4418 | V_loss: 1.02723, V_accuracy: 79.0000 | V_loss decreased (1.04963 --> 1.02723). Saving model... | T_time: 00:26:37, T_speed: 0.150\n",
      "[Epoch 250] T_loss: 1.44824, T_accuracy: 81.2636 | V_loss: 1.01188, V_accuracy: 79.6200 | V_loss decreased (1.02723 --> 1.01188). Saving model... | T_time: 00:27:44, T_speed: 0.150\n",
      "[Epoch 260] T_loss: 1.44808, T_accuracy: 81.1127 | V_loss: 0.99983, V_accuracy: 79.7800 | V_loss decreased (1.01188 --> 0.99983). Saving model... | T_time: 00:28:50, T_speed: 0.150\n",
      "[Epoch 270] T_loss: 1.41652, T_accuracy: 81.5691 | V_loss: 0.98996, V_accuracy: 79.8600 | V_loss decreased (0.99983 --> 0.98996). Saving model... | T_time: 00:29:57, T_speed: 0.150\n",
      "[Epoch 280] T_loss: 1.41387, T_accuracy: 81.5364 | V_loss: 1.01080, V_accuracy: 79.1200 | Early stopping counter: 1 out of 5 | T_time: 00:31:03, T_speed: 0.150\n",
      "[Epoch 290] T_loss: 1.39525, T_accuracy: 81.8200 | V_loss: 0.97722, V_accuracy: 80.1800 | V_loss decreased (0.98996 --> 0.97722). Saving model... | T_time: 00:32:09, T_speed: 0.150\n",
      "[Epoch 300] T_loss: 1.38132, T_accuracy: 81.9655 | V_loss: 0.97192, V_accuracy: 80.1800 | V_loss decreased (0.97722 --> 0.97192). Saving model... | T_time: 00:33:16, T_speed: 0.150\n",
      "[Epoch 310] T_loss: 0.71532, T_accuracy: 82.8927 | V_loss: 0.50553, V_accuracy: 81.0400 | V_loss decreased (0.97192 --> 0.50553). Saving model... | T_time: 00:34:23, T_speed: 0.150\n",
      "[Epoch 320] T_loss: 0.68605, T_accuracy: 84.8127 | V_loss: 0.49071, V_accuracy: 83.0600 | V_loss decreased (0.50553 --> 0.49071). Saving model... | T_time: 00:35:29, T_speed: 0.150\n",
      "[Epoch 330] T_loss: 0.67150, T_accuracy: 85.2455 | V_loss: 0.48604, V_accuracy: 82.9600 | V_loss decreased (0.49071 --> 0.48604). Saving model... | T_time: 00:36:37, T_speed: 0.150\n",
      "[Epoch 340] T_loss: 0.65308, T_accuracy: 85.8091 | V_loss: 0.47608, V_accuracy: 83.7200 | V_loss decreased (0.48604 --> 0.47608). Saving model... | T_time: 00:37:51, T_speed: 0.150\n",
      "[Epoch 350] T_loss: 0.64472, T_accuracy: 85.8273 | V_loss: 0.45824, V_accuracy: 83.8400 | V_loss decreased (0.47608 --> 0.45824). Saving model... | T_time: 00:39:04, T_speed: 0.149\n",
      "[Epoch 360] T_loss: 0.63255, T_accuracy: 86.1164 | V_loss: 0.46866, V_accuracy: 83.7600 | Early stopping counter: 1 out of 5 | T_time: 00:40:17, T_speed: 0.149\n",
      "[Epoch 370] T_loss: 0.62194, T_accuracy: 86.5073 | V_loss: 0.45748, V_accuracy: 83.9600 | V_loss decreased (0.45824 --> 0.45748). Saving model... | T_time: 00:41:30, T_speed: 0.149\n",
      "[Epoch 380] T_loss: 0.60789, T_accuracy: 86.7691 | V_loss: 0.44265, V_accuracy: 84.6000 | V_loss decreased (0.45748 --> 0.44265). Saving model... | T_time: 00:42:41, T_speed: 0.148\n",
      "[Epoch 390] T_loss: 0.60157, T_accuracy: 86.9600 | V_loss: 0.43430, V_accuracy: 84.8000 | V_loss decreased (0.44265 --> 0.43430). Saving model... | T_time: 00:43:53, T_speed: 0.148\n",
      "[Epoch 400] T_loss: 0.59648, T_accuracy: 87.0091 | V_loss: 0.44359, V_accuracy: 84.3600 | Early stopping counter: 1 out of 5 | T_time: 00:45:02, T_speed: 0.148\n",
      "[Epoch 410] T_loss: 0.58607, T_accuracy: 87.3891 | V_loss: 0.43272, V_accuracy: 85.0800 | V_loss decreased (0.43430 --> 0.43272). Saving model... | T_time: 00:46:07, T_speed: 0.148\n",
      "[Epoch 420] T_loss: 0.57548, T_accuracy: 87.6055 | V_loss: 0.43730, V_accuracy: 84.8000 | Early stopping counter: 1 out of 5 | T_time: 00:47:12, T_speed: 0.148\n",
      "[Epoch 430] T_loss: 0.57736, T_accuracy: 87.5200 | V_loss: 0.43477, V_accuracy: 84.9200 | Early stopping counter: 2 out of 5 | T_time: 00:48:18, T_speed: 0.148\n",
      "[Epoch 440] T_loss: 0.56314, T_accuracy: 87.8127 | V_loss: 0.42266, V_accuracy: 85.4000 | V_loss decreased (0.43272 --> 0.42266). Saving model... | T_time: 00:49:23, T_speed: 0.148\n",
      "[Epoch 450] T_loss: 0.55449, T_accuracy: 88.0655 | V_loss: 0.41424, V_accuracy: 85.6000 | V_loss decreased (0.42266 --> 0.41424). Saving model... | T_time: 00:50:28, T_speed: 0.149\n",
      "[Epoch 460] T_loss: 0.55322, T_accuracy: 88.0673 | V_loss: 0.40835, V_accuracy: 85.9000 | V_loss decreased (0.41424 --> 0.40835). Saving model... | T_time: 00:51:34, T_speed: 0.149\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    config = {\n",
    "        'epochs': 10_000,\n",
    "        'batch_size': 2048,\n",
    "        'validation_intervals': 10,\n",
    "        'learning_rate': 2e-5,\n",
    "        'early_stop_patience': 5,\n",
    "        'early_stop_delta': 0.00001,\n",
    "        'weight_decay': 0.001,\n",
    "        'project': \"CSE533_Fashion_MNIST\",\n",
    "        'use_wandb': True,\n",
    "        'current_time_str': datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        'checkpoints_path': \"checkpoints\"\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(config['checkpoints_path']):\n",
    "        os.makedirs(config['checkpoints_path'])\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if config['use_wandb'] else \"disabled\",\n",
    "        project=config['project'],\n",
    "        notes=\"Homework3\",\n",
    "        tags=['googlenet', 'FashionMNIST'],\n",
    "        name=config['current_time_str'],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    print(wandb.config)\n",
    "\n",
    "    mean = 0\n",
    "    std = 0\n",
    "\n",
    "    try:\n",
    "        mean, std = model_train(config)\n",
    "    except:\n",
    "        wandb.finish()\n",
    "        return\n",
    "        \n",
    "    model_test(config, mean, std)\n",
    "\n",
    "    print(\"#### Q1 Answer ####\")\n",
    "    print(\"Fashion Mnist Train Data's Mean\\t:\", mean)\n",
    "    print(\"Fashion Mnist Train Data's Std\\t:\", std)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE533",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
