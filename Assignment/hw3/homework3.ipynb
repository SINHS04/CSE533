{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from string import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions from _01_code/_99_common_utils/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_linux():\n",
    "    if sys.platform.startswith(\"linux\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_windows():\n",
    "    if os.name == \"nt\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_mac():\n",
    "    if sys.platform == \"darwin\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_num_cpu_cores():\n",
    "    import multiprocessing\n",
    "    return multiprocessing.cpu_count()\n",
    "\n",
    "class DeltaTemplate(Template):\n",
    "    delimiter = \"%\"\n",
    "\n",
    "    def strfdelta(tdelta, fmt):\n",
    "        d = {\"D\": tdelta.days}\n",
    "        d[\"H\"], rem = divmod(tdelta.seconds, 3600)\n",
    "        d[\"M\"], d[\"S\"] = divmod(rem, 60)\n",
    "        t = DeltaTemplate(fmt)\n",
    "        return t.substitute(**d)\n",
    "\n",
    "def strfdelta(td, fmt):\n",
    "\n",
    "    # Get the timedelta’s sign and absolute number of seconds.\n",
    "    sign = \"-\" if td.days < 0 else \"+\"\n",
    "    secs = abs(td).total_seconds()\n",
    "\n",
    "    # Break the seconds into more readable quantities.\n",
    "    days, rem = divmod(secs, 86400)  # Seconds per day: 24 * 60 * 60\n",
    "    hours, rem = divmod(rem, 3600)  # Seconds per hour: 60 * 60\n",
    "    mins, secs = divmod(rem, 60)\n",
    "\n",
    "    # Format (as per above answers) and return the result string.\n",
    "    t = DeltaTemplate(fmt)\n",
    "    return t.substitute(\n",
    "        s=sign,\n",
    "        D=\"{:d}\".format(int(days)),\n",
    "        H=\"{:02d}\".format(int(hours)),\n",
    "        M=\"{:02d}\".format(int(mins)),\n",
    "        S=\"{:02d}\".format(int(secs)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [문제 1] Fashion MNIST 데이터 정규화를 위한 Mean과 Std값 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment function\n",
    "def augment_f_mnist_train(f_mnist_train):\n",
    "    f_mnist_train_transforms = nn.Sequential(\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop([28, 28], padding=4),\n",
    "    )\n",
    "\n",
    "    transformed_train_data = []\n",
    "\n",
    "    for image, label in f_mnist_train:\n",
    "        transformed_image = f_mnist_train_transforms(image)\n",
    "        transformed_train_data.append((transformed_image, label))\n",
    "\n",
    "    f_mnist_train = ConcatDataset([f_mnist_train, transformed_train_data])\n",
    "\n",
    "    return f_mnist_train\n",
    "\n",
    "def get_f_mnist_train_data():\n",
    "    data_path = \".\" \n",
    "\n",
    "    f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "    f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "    ## augment ##\n",
    "    f_mnist_train = augment_f_mnist_train(f_mnist_train)\n",
    "    ##############\n",
    "\n",
    "    imgs = [img for img, _ in f_mnist_train]\n",
    "    imgs = torch.concat(imgs, dim=0)\n",
    "    mean = imgs.mean(dim=[0, 1, 2])\n",
    "    std = imgs.std(dim=[0, 1, 2])\n",
    "\n",
    "    num_data_loading_workers = get_num_cpu_cores() if is_linux() or is_windows() else 0\n",
    "\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=f_mnist_train, batch_size=wandb.config.batch_size, shuffle=True,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=f_mnist_validation, batch_size=wandb.config.batch_size,\n",
    "        pin_memory=True, num_workers=num_data_loading_workers\n",
    "    )\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    )\n",
    "    \n",
    "    return train_data_loader, validation_data_loader, f_mnist_transforms, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_mnist_test_data(mean, std):\n",
    "    data_path = \".\"\n",
    "    f_mnist_test_images = datasets.FashionMNIST(data_path, train=False, download=True)\n",
    "    f_mnist_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    test_data_loader = DataLoader(dataset=f_mnist_test, batch_size=len(f_mnist_test))\n",
    "\n",
    "    f_mnist_transforms = nn.Sequential(\n",
    "        transforms.ConvertImageDtype(torch.float),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    )\n",
    "\n",
    "    return f_mnist_test_images, test_data_loader, f_mnist_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNet(CNN) Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_googlenet_model():\n",
    "    class Inception(nn.Module):\n",
    "        def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "            super(Inception, self).__init__(**kwargs)\n",
    "            self.b1_1 = nn.LazyConv2d(out_channels=c1, kernel_size=1)\n",
    "\n",
    "            self.b2_1 = nn.LazyConv2d(out_channels=c2[0], kernel_size=1)\n",
    "            self.b2_2 = nn.LazyConv2d(out_channels=c2[1], kernel_size=3, padding=1)\n",
    "\n",
    "            self.b3_1 = nn.LazyConv2d(out_channels=c3[0], kernel_size=1)\n",
    "            self.b3_2 = nn.LazyConv2d(out_channels=c3[1], kernel_size=5, padding=2)\n",
    "\n",
    "            self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "            self.b4_2 = nn.LazyConv2d(out_channels=c4, kernel_size=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            b1 = torch.relu(self.b1_1(x))\n",
    "            b2 = torch.relu(self.b2_2(torch.relu(self.b2_1(x))))\n",
    "            b3 = torch.relu(self.b3_2(torch.relu(self.b3_1(x))))\n",
    "            b4 = torch.relu(self.b4_2(self.b4_1(x)))\n",
    "            return torch.cat((b1, b2, b3, b4), dim=1)\n",
    "        \n",
    "    class InceptionAux(nn.Module):\n",
    "        def __init__(self, n_outputs, **kwargs):\n",
    "            super(InceptionAux, self).__init__(**kwargs)\n",
    "\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=128, kernel_size=1),\n",
    "            )\n",
    "\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.LazyLinear(out_features=1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "                nn.LazyLinear(out_features=n_outputs),\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv(x)\n",
    "            x = x.view(x.shape[0], -1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "        \n",
    "    class GoogleNet(nn.Module):\n",
    "        def __init__(self, n_outputs=10):\n",
    "            super(GoogleNet, self).__init__()\n",
    "            self.conv_block = nn.Sequential(self.conv_blk_1(), self.conv_blk_2())\n",
    "            self.inception_block_1 = self.inception_blk_1()\n",
    "            self.inception_block_2 = self.inception_blk_2()\n",
    "            self.inception_block_3 = self.inception_blk_3()\n",
    "            self.aux_1 = InceptionAux(n_outputs)\n",
    "            self.aux_2 = InceptionAux(n_outputs)\n",
    "\n",
    "        def conv_blk_1(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "        def conv_blk_2(self):\n",
    "            return nn.Sequential(\n",
    "                nn.LazyConv2d(out_channels=64, kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.LazyConv2d(out_channels=192, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        \n",
    "        def inception_blk_1(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=64, c2=(96, 128), c3=(16, 32), c4=32),\n",
    "                Inception(c1=128, c2=(128, 192), c3=(32, 96), c4=64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                Inception(c1=192, c2=(96, 208), c3=(16, 48), c4=64),\n",
    "            )\n",
    "        \n",
    "        def inception_blk_2(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=160, c2=(112, 224), c3=(24, 64), c4=64),\n",
    "                Inception(c1=128, c2=(128, 256), c3=(24, 64), c4=64),\n",
    "                Inception(c1=112, c2=(144, 288), c3=(32, 64), c4=64),\n",
    "            )\n",
    "        \n",
    "        def inception_blk_3(self):\n",
    "            return nn.Sequential(\n",
    "                Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                Inception(c1=256, c2=(160, 320), c3=(32, 128), c4=128),\n",
    "                Inception(c1=384, c2=(192, 384), c3=(48, 128), c4=128),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.conv_block(x)\n",
    "            x = self.inception_block_1(x)\n",
    "            aux_out_1 = self.aux_1(x)\n",
    "            x = self.inception_block_2(x)\n",
    "            aux_out_2 = self.aux_2(x)\n",
    "            x = self.inception_block_3(x)\n",
    "            return x, aux_out_1, aux_out_2\n",
    "\n",
    "    my_model = GoogleNet()\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "  def __init__(self, patience=10, delta=0.00001, project_name=None, checkpoint_file_path=None, run_time_str=None):\n",
    "    self.patience = patience\n",
    "    self.counter = 0\n",
    "    self.delta = delta\n",
    "\n",
    "    self.val_loss_min = None\n",
    "    self.file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_{run_time_str}.pt\"\n",
    "    )\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "  def check_and_save(self, new_validation_loss, model):\n",
    "    early_stop = False\n",
    "\n",
    "    if self.val_loss_min is None:\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      message = f'Early stopping is stated!'\n",
    "    elif new_validation_loss < self.val_loss_min - self.delta:\n",
    "      message = f'V_loss decreased ({self.val_loss_min:7.5f} --> {new_validation_loss:7.5f}). Saving model...'\n",
    "      self.save_checkpoint(new_validation_loss, model)\n",
    "      self.val_loss_min = new_validation_loss\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      message = f'Early stopping counter: {self.counter} out of {self.patience}'\n",
    "      if self.counter >= self.patience:\n",
    "        early_stop = True\n",
    "        message += \" *** TRAIN EARLY STOPPED! ***\"\n",
    "\n",
    "    return message, early_stop\n",
    "\n",
    "  def save_checkpoint(self, val_loss, model):\n",
    "    '''Saves model when validation loss decrease.'''\n",
    "    torch.save(model.state_dict(), self.file_path)\n",
    "    torch.save(model.state_dict(), self.latest_file_path)\n",
    "    self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTrainer:\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.train_data_loader = train_data_loader\n",
    "    self.validation_data_loader = validation_data_loader\n",
    "    self.transforms = transforms\n",
    "    self.run_time_str = run_time_str\n",
    "    self.wandb = wandb\n",
    "    self.device = device\n",
    "    self.checkpoint_file_path = checkpoint_file_path\n",
    "\n",
    "    # Use a built-in loss function\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      if self.transforms:\n",
    "        input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train = self.model(input_train)\n",
    "\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        if self.transforms:\n",
    "          input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation = self.model(input_validation)\n",
    "        loss_validation += self.loss_fn(output_validation, target_validation).item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy\n",
    "\n",
    "  def train_loop(self):\n",
    "    early_stopping = EarlyStopping(\n",
    "      patience=self.wandb.config.early_stop_patience,\n",
    "      delta=self.wandb.config.early_stop_delta,\n",
    "      project_name=self.project_name,\n",
    "      checkpoint_file_path=self.checkpoint_file_path,\n",
    "      run_time_str=self.run_time_str\n",
    "    )\n",
    "    n_epochs = self.wandb.config.epochs\n",
    "    training_start_time = datetime.now()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "      train_loss, train_accuracy = self.do_train()\n",
    "\n",
    "      if epoch == 1 or epoch % self.wandb.config.validation_intervals == 0:\n",
    "        validation_loss, validation_accuracy = self.do_validation()\n",
    "\n",
    "        elapsed_time = datetime.now() - training_start_time\n",
    "        epoch_per_second = 0 if elapsed_time.seconds == 0 else epoch / elapsed_time.seconds\n",
    "\n",
    "        message, early_stop = early_stopping.check_and_save(validation_loss, self.model)\n",
    "\n",
    "        print(\n",
    "          f\"[Epoch {epoch:>3}] \"\n",
    "          f\"T_loss: {train_loss:7.5f}, \"\n",
    "          f\"T_accuracy: {train_accuracy:6.4f} | \"\n",
    "          f\"V_loss: {validation_loss:7.5f}, \"\n",
    "          f\"V_accuracy: {validation_accuracy:6.4f} | \"\n",
    "          f\"{message} | \"\n",
    "          f\"T_time: {strfdelta(elapsed_time, '%H:%M:%S')}, \"\n",
    "          f\"T_speed: {epoch_per_second:4.3f}\"\n",
    "        )\n",
    "\n",
    "        self.wandb.log({\n",
    "          \"Epoch\": epoch,\n",
    "          \"Training loss\": train_loss,\n",
    "          \"Training accuracy (%)\": train_accuracy,\n",
    "          \"Validation loss\": validation_loss,\n",
    "          \"Validation accuracy (%)\": validation_accuracy,\n",
    "          \"Training speed (epochs/sec.)\": epoch_per_second,\n",
    "        })\n",
    "\n",
    "        if early_stop:\n",
    "          break\n",
    "\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    print(f\"Final training time: {strfdelta(elapsed_time, '%H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationTester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTester:\n",
    "  def __init__(self, project_name, model, test_data_loader, transforms, checkpoint_file_path):\n",
    "    self.project_name = project_name\n",
    "    self.model = model\n",
    "    self.test_data_loader = test_data_loader\n",
    "    self.transforms = transforms\n",
    "\n",
    "    self.latest_file_path = os.path.join(\n",
    "      checkpoint_file_path, f\"{project_name}_checkpoint_latest.pt\"\n",
    "    )\n",
    "\n",
    "    print(\"MODEL FILE: {0}\".format(self.latest_file_path))\n",
    "\n",
    "    self.model.load_state_dict(torch.load(self.latest_file_path, map_location=torch.device('cpu')))\n",
    "\n",
    "  def test(self):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    num_corrects_test = 0\n",
    "    num_tested_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for test_batch in self.test_data_loader:\n",
    "        input_test, target_test = test_batch\n",
    "\n",
    "        input_test = self.transforms(input_test)\n",
    "\n",
    "        output_test, _, _ = self.model(input_test)\n",
    "\n",
    "        predicted_test = torch.argmax(output_test, dim=1)\n",
    "        num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "        num_tested_samples += len(input_test)\n",
    "\n",
    "      test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "    print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "  def test_single(self, input_test):\n",
    "    self.model.eval()    # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    with torch.no_grad():\n",
    "      input_test = self.transforms(input_test)\n",
    "\n",
    "      output_test, _, _ = self.model(input_test)\n",
    "      predicted_test = torch.argmax(output_test, dim=1)\n",
    "\n",
    "    return predicted_test.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNetClassificationTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNetClassificationTrainer(ClassificationTrainer):\n",
    "  def __init__(\n",
    "    self, project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "    run_time_str, wandb, device, checkpoint_file_path\n",
    "  ):\n",
    "    super(GoogLeNetClassificationTrainer, self).__init__(\n",
    "      project_name, model, optimizer, train_data_loader, validation_data_loader, transforms,\n",
    "      run_time_str, wandb, device, checkpoint_file_path\n",
    "    )\n",
    "\n",
    "  def do_train(self):\n",
    "    self.model.train()  # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_train = 0.0\n",
    "    num_corrects_train = 0\n",
    "    num_trained_samples = 0\n",
    "    num_trains = 0\n",
    "\n",
    "    for train_batch in self.train_data_loader:\n",
    "      input_train, target_train = train_batch\n",
    "      input_train = input_train.to(device=self.device)\n",
    "      target_train = target_train.to(device=self.device)\n",
    "\n",
    "      input_train = self.transforms(input_train)\n",
    "\n",
    "      output_train, output_train_ax_1, output_train_ax_2 = self.model(input_train)\n",
    "      loss = self.loss_fn(output_train, target_train)\n",
    "      loss_aux_1 = self.loss_fn(output_train_ax_1, target_train)\n",
    "      loss_aux_2 = self.loss_fn(output_train_ax_2, target_train)\n",
    "      loss += 0.3 * (loss_aux_1 + loss_aux_2)\n",
    "\n",
    "      loss_train += loss.item()\n",
    "\n",
    "      predicted_train = torch.argmax(output_train, dim=1)\n",
    "      num_corrects_train += torch.sum(torch.eq(predicted_train, target_train)).item()\n",
    "\n",
    "      num_trained_samples += len(input_train)\n",
    "      num_trains += 1\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "    train_loss = loss_train / num_trains\n",
    "    train_accuracy = 100.0 * num_corrects_train / num_trained_samples\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "  def do_validation(self):\n",
    "    self.model.eval()   # Explained at 'Diverse Techniques' section\n",
    "\n",
    "    loss_validation = 0.0\n",
    "    num_corrects_validation = 0\n",
    "    num_validated_samples = 0\n",
    "    num_validations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for validation_batch in self.validation_data_loader:\n",
    "        input_validation, target_validation = validation_batch\n",
    "        input_validation = input_validation.to(device=self.device)\n",
    "        target_validation = target_validation.to(device=self.device)\n",
    "\n",
    "        input_validation = self.transforms(input_validation)\n",
    "\n",
    "        output_validation, output_validation_ax_1, output_validation_ax_2 = self.model(input_validation)\n",
    "        loss_validation = self.loss_fn(output_validation, target_validation)\n",
    "        loss_validation_aux_1 = self.loss_fn(output_validation_ax_1, target_validation)\n",
    "        loss_validation_aux_2 = self.loss_fn(output_validation_ax_2, target_validation)\n",
    "        loss_validation += 0.3 * (loss_validation_aux_1 + loss_validation_aux_2)\n",
    "        loss_validation += loss_validation.item()\n",
    "\n",
    "        predicted_validation = torch.argmax(output_validation, dim=1)\n",
    "        num_corrects_validation += torch.sum(torch.eq(predicted_validation, target_validation)).item()\n",
    "\n",
    "        num_validated_samples += len(input_validation)\n",
    "        num_validations += 1\n",
    "\n",
    "    validation_loss = loss_validation / num_validations\n",
    "    validation_accuracy = 100.0 * num_corrects_validation / num_validated_samples\n",
    "\n",
    "    return validation_loss, validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(config):\n",
    "    train_data_loader, validation_data_loader, f_mnist_transforms, mean, std = get_f_mnist_train_data()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = get_googlenet_model()\n",
    "    best_path = 'checkpoints/CSE533_Fashion_MNIST_checkpoint_2023-11-18_00-17-29.pt'\n",
    "    model.load_state_dict(torch.load(best_path, map_location=torch.device('cpu')))\n",
    "    model.to(device)\n",
    "\n",
    "    summary(\n",
    "        model = model,\n",
    "        input_size=(1, 1, 28, 28),\n",
    "        col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\", \"mult_adds\"]\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=wandb.config.learning_rate,\n",
    "        weight_decay=0.001\n",
    "    )\n",
    "\n",
    "    classification_trainer = GoogLeNetClassificationTrainer(\n",
    "        config['project'], model, optimizer, train_data_loader, validation_data_loader, f_mnist_transforms,\n",
    "        config['current_time_str'], wandb, device, config['checkpoints_path']\n",
    "    )\n",
    "\n",
    "    classification_trainer.train_loop()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(config, mean, std):\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_f_mnist_test_data(mean, std)\n",
    "\n",
    "    test_model = get_googlenet_model()\n",
    "\n",
    "    classification_tester = ClassificationTester(\n",
    "        config['project'], test_model, test_data_loader, f_mnist_transforms, config['checkpoints_path']\n",
    "    )\n",
    "    classification_tester.test()\n",
    "\n",
    "    img, label = f_mnist_test_images[0]\n",
    "    print(\"     LABEL:\", label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    output = classification_tester.test_single(\n",
    "        torch.tensor(np.array(f_mnist_test_images[0][0])).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "    )\n",
    "    print(\"PREDICTION:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mckw6896\u001b[0m (\u001b[33mdicelab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nlp-06/CSE533/assignment/CSE533/Assignment/hw3/wandb/run-20231118_042801-e64no61t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t' target=\"_blank\">2023-11-18_04-28-00</a></strong> to <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 10000, 'batch_size': 2048, 'validation_intervals': 10, 'learning_rate': 2e-05, 'early_stop_patience': 10, 'early_stop_delta': 1e-05, 'weight_decay': 0, 'project': 'CSE533_Fashion_MNIST', 'use_wandb': True, 'current_time_str': '2023-11-18_04-28-00', 'checkpoints_path': 'checkpoints'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-06/anaconda3/envs/CSE533/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   1] T_loss: 0.48847, T_accuracy: 88.9809 | V_loss: 0.19601, V_accuracy: 93.6000 | Early stopping is stated! | T_time: 00:00:12, T_speed: 0.083\n",
      "[Epoch  10] T_loss: 0.44590, T_accuracy: 90.0955 | V_loss: 0.19195, V_accuracy: 93.7000 | V_loss decreased (0.19601 --> 0.19195). Saving model... | T_time: 00:01:48, T_speed: 0.093\n",
      "[Epoch  20] T_loss: 0.42392, T_accuracy: 90.6145 | V_loss: 0.19077, V_accuracy: 93.6600 | V_loss decreased (0.19195 --> 0.19077). Saving model... | T_time: 00:03:35, T_speed: 0.093\n",
      "[Epoch  30] T_loss: 0.40500, T_accuracy: 91.1409 | V_loss: 0.19221, V_accuracy: 93.6600 | Early stopping counter: 1 out of 10 | T_time: 00:05:21, T_speed: 0.093\n",
      "[Epoch  40] T_loss: 0.38771, T_accuracy: 91.6091 | V_loss: 0.19441, V_accuracy: 93.6400 | Early stopping counter: 2 out of 10 | T_time: 00:07:07, T_speed: 0.094\n",
      "[Epoch  50] T_loss: 0.37059, T_accuracy: 92.1345 | V_loss: 0.19416, V_accuracy: 93.6200 | Early stopping counter: 3 out of 10 | T_time: 00:08:54, T_speed: 0.094\n",
      "[Epoch  60] T_loss: 0.35532, T_accuracy: 92.6045 | V_loss: 0.19894, V_accuracy: 93.6800 | Early stopping counter: 4 out of 10 | T_time: 00:10:43, T_speed: 0.093\n",
      "[Epoch  70] T_loss: 0.33886, T_accuracy: 93.0455 | V_loss: 0.20432, V_accuracy: 93.7200 | Early stopping counter: 5 out of 10 | T_time: 00:12:38, T_speed: 0.092\n",
      "[Epoch  80] T_loss: 0.32301, T_accuracy: 93.4945 | V_loss: 0.20302, V_accuracy: 93.6400 | Early stopping counter: 6 out of 10 | T_time: 00:14:34, T_speed: 0.092\n",
      "[Epoch  90] T_loss: 0.30686, T_accuracy: 93.9736 | V_loss: 0.20598, V_accuracy: 93.7400 | Early stopping counter: 7 out of 10 | T_time: 00:16:30, T_speed: 0.091\n",
      "[Epoch 100] T_loss: 0.29026, T_accuracy: 94.3818 | V_loss: 0.21098, V_accuracy: 93.6200 | Early stopping counter: 8 out of 10 | T_time: 00:18:27, T_speed: 0.090\n",
      "[Epoch 110] T_loss: 0.27415, T_accuracy: 94.8373 | V_loss: 0.21316, V_accuracy: 93.5600 | Early stopping counter: 9 out of 10 | T_time: 00:20:25, T_speed: 0.090\n",
      "[Epoch 120] T_loss: 0.25870, T_accuracy: 95.2873 | V_loss: 0.22181, V_accuracy: 93.4800 | Early stopping counter: 10 out of 10 *** TRAIN EARLY STOPPED! *** | T_time: 00:22:22, T_speed: 0.089\n",
      "Final training time: 00:22:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▃▄▄▅▆▆▇▇█</td></tr><tr><td>Training accuracy (%)</td><td>▁▂▃▃▄▅▅▆▆▇▇██</td></tr><tr><td>Training loss</td><td>█▇▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▁▇█████▇▇▆▆▅▅</td></tr><tr><td>Validation accuracy (%)</td><td>▄▇▆▆▅▅▆▇▅█▅▃▁</td></tr><tr><td>Validation loss</td><td>▂▁▁▁▂▂▃▄▄▄▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>120</td></tr><tr><td>Training accuracy (%)</td><td>95.28727</td></tr><tr><td>Training loss</td><td>0.2587</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.08942</td></tr><tr><td>Validation accuracy (%)</td><td>93.48</td></tr><tr><td>Validation loss</td><td>0.22181</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2023-11-18_04-28-00</strong> at: <a href='https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t' target=\"_blank\">https://wandb.ai/dicelab/CSE533_Fashion_MNIST/runs/e64no61t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231118_042801-e64no61t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-06/anaconda3/envs/CSE533/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: checkpoints/CSE533_Fashion_MNIST_checkpoint_latest.pt\n",
      "TEST RESULTS: 90.390%\n",
      "     LABEL: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION: 9\n",
      "#### Q1 Answer ####\n",
      "Fashion Mnist Train Data's Mean\t: tensor(0.2783)\n",
      "Fashion Mnist Train Data's Std\t: tensor(0.3517)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    config = {\n",
    "        'epochs': 10_000,\n",
    "        'batch_size': 2048,\n",
    "        'validation_intervals': 10,\n",
    "        'learning_rate': 2e-5,\n",
    "        'early_stop_patience': 10,\n",
    "        'early_stop_delta': 0.00001,\n",
    "        'weight_decay': 0,\n",
    "        'project': \"CSE533_Fashion_MNIST\",\n",
    "        'use_wandb': True,\n",
    "        'current_time_str': datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S'),\n",
    "        'checkpoints_path': \"checkpoints\"\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(config['checkpoints_path']):\n",
    "        os.makedirs(config['checkpoints_path'])\n",
    "\n",
    "    wandb.init(\n",
    "        mode=\"online\" if config['use_wandb'] else \"disabled\",\n",
    "        project=config['project'],\n",
    "        notes=\"Homework3\",\n",
    "        tags=['googlenet', 'FashionMNIST'],\n",
    "        name=config['current_time_str'],\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    print(wandb.config)\n",
    "\n",
    "    mean = 0.3\n",
    "    std = 0.2\n",
    "\n",
    "    try:\n",
    "        mean, std = model_train(config)\n",
    "    except:\n",
    "        wandb.finish()\n",
    "        return\n",
    "        \n",
    "    model_test(config, mean, std)\n",
    "\n",
    "    print(\"#### Q1 Answer ####\")\n",
    "    print(\"Fashion Mnist Train Data's Mean\\t:\", mean)\n",
    "    print(\"Fashion Mnist Train Data's Std\\t:\", std)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE533",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
